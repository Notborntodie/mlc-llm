PrimFunc name: reshape7
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_reshape: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_excluding_cache, T.int64(12288)), "float16")
    T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), num_tokens_excluding_cache, T.int64(96), T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(12), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(12288))
                v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(12288) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(A[T.int64(0), v0, v1 * T.int64(128) + v2])
                T.writes(T_reshape[T.int64(0), v0, v1, v2])
                T_reshape[T.int64(0), v0, v1, v2] = A[T.int64(0), v0, v1 * T.int64(128) + v2]
================================================================================
PrimFunc name: fused_fused_decode1_fused_NT_matmul5_cast2
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv803: T.handle, p_lv804: T.handle, lv3215: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int64()
    lv803 = T.match_buffer(p_lv803, (vocab_size, T.int64(1024)), "uint32")
    lv804 = T.match_buffer(p_lv804, (vocab_size, T.int64(128)), "float16")
    compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1), vocab_size))
    # with T.block("root"):
    NT_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(1), vocab_size), "float16", scope="local")
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1), T.int64(1), vocab_size), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(64), T.int64(1), T.int64(1), vocab_size), "float16", scope="local")
    lv803_local = T.alloc_buffer((vocab_size, T.int64(1024)), "uint32", scope="local")
    lv3215_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="shared")
    for u_fused_ax0_fused_fused_0 in T.thread_binding(vocab_size, thread="blockIdx.x"):
        for u_fused_ax0_fused_fused_1 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2_0 in T.serial(T.int64(8), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax2_1 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
                            for ax2_2 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax2_3 in T.vectorized(T.int64(8)):
                                    with T.block("lv3215_shared"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2 = T.axis.spatial(T.int64(4096), ax2_0 * T.int64(512) + ax2_1 * T.int64(512) + ax2_2 * T.int64(8) + ax2_3)
                                        T.reads(lv3215[v0, v1, v2])
                                        T.writes(lv3215_shared[v0, v1, v2])
                                        lv3215_shared[v0, v1, v2] = lv3215[v0, v1, v2]
                for u_fused_ax0_fused_fused_2_init in range(T.int64(1)):
                    for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init in T.vectorized(T.int64(4)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init)
                            v0 = T.axis.spatial(vocab_size, u_fused_ax0_fused_fused_0 + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = T.float16(0)
                for ax1_0_fused_ax1_1_fused_0 in T.serial(T.int64(16), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0_0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax0_1 in T.vectorized(T.int64(1)):
                            with T.block("lv803_local"):
                                v0 = T.axis.spatial(vocab_size, u_fused_ax0_fused_fused_0 + ax0_0 + ax0_1)
                                v1 = T.axis.spatial(T.int64(1024), ax1_0_fused_ax1_1_fused_0 * T.int64(64) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 + ax1)
                                T.reads(lv803[v0, v1])
                                T.writes(lv803_local[v0, v1])
                                lv803_local[v0, v1] = lv803[v0, v1]
                    for u_fused_ax0_fused_fused_2, ax1_0_fused_ax1_1_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 in T.vectorized(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1)
                                v0 = T.axis.spatial(vocab_size, u_fused_ax0_fused_fused_0 + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2)
                                vax1_0_fused_ax1_1_fused_2, vax1_0_fused_ax1_1_fused_0 = T.axis.remap("RR", [ax1_0_fused_ax1_1_fused_2, ax1_0_fused_ax1_1_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0], lv3215_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused], lv803_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], lv804[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
                                T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] + lv3215_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused] * ((T.Cast("float16", T.bitwise_and(T.shift_right(lv803_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], T.Cast("uint32", (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv804[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
        for ax2_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, v0 = T.axis.remap("SS", [ax0, u_fused_ax0_fused_fused_0])
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = T.float16(0)
                        for ax1 in range(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, v0 = T.axis.remap("SRS", [ax0, ax1, u_fused_ax0_fused_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0], NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0]
        for ax1_fused_1 in range(T.int64(1)):
            for ax1_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, v0 = T.axis.remap("RS", [ax0, u_fused_ax0_fused_fused_0])
                        T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                        T.writes(NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                        with T.init():
                            NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = T.float16(0)
                        NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0]
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax0_fused_1 in range(T.int64(1)):
                with T.block("compute"):
                    v0 = T.axis.spatial(vocab_size, u_fused_ax0_fused_fused_0)
                    T.reads(NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                    T.writes(compute_intermediate_intermediate[T.int64(0), T.int64(0), v0])
                    compute_intermediate_intermediate[T.int64(0), T.int64(0), v0] = T.Cast("float32", NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
================================================================================
PrimFunc name: fused_split_silu_multiply
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv3: T.Buffer((T.int64(1), T.int64(1), T.int64(22016)), "float16"), T_multiply_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(11008)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding(T.int64(11), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_multiply_1"):
                v0 = T.axis.spatial(T.int64(11008), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(11008))
                T.reads(lv3[T.int64(0), T.int64(0), v0:v0 + T.int64(11009)])
                T.writes(T_multiply_intermediate_1[T.int64(0), T.int64(0), v0])
                T_multiply_intermediate_1[T.int64(0), T.int64(0), v0] = lv3[T.int64(0), T.int64(0), v0] * T.sigmoid(lv3[T.int64(0), T.int64(0), v0]) * lv3[T.int64(0), T.int64(0), v0 + T.int64(11008)]
================================================================================
PrimFunc name: divide
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, B: T.Buffer((), "float32"), var_T_divide: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(1), vocab_size))
    T_divide = T.match_buffer(var_T_divide, (T.int64(1), T.int64(1), vocab_size))
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding((vocab_size + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_divide"):
                v0 = T.axis.spatial(vocab_size, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < vocab_size)
                T.reads(A[T.int64(0), T.int64(0), v0], B[()])
                T.writes(T_divide[T.int64(0), T.int64(0), v0])
                T_divide[T.int64(0), T.int64(0), v0] = A[T.int64(0), T.int64(0), v0] / B[()]
================================================================================
PrimFunc name: matmul11
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_B: T.handle, var_matmul: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache, num_tokens_including_cache = T.int64(), T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(32), num_tokens_excluding_cache, num_tokens_including_cache), "float16")
    B = T.match_buffer(var_B, (T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16")
    matmul = T.match_buffer(var_matmul, (T.int64(1), T.int64(32), num_tokens_excluding_cache, T.int64(128)), "float16")
    # with T.block("root"):
    A_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64)), "float16", scope="shared.dyn")
    B_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), T.int64(128), (num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64)), "float16", scope="shared.dyn")
    A_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64)), "float16", scope="wmma.matrix_a")
    B_reindex_pad_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(32), T.int64(128), (num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64)), "float16", scope="wmma.matrix_b")
    matmul_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="shared.dyn")
    matmul_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(32), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(1), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(32), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial(T.int64(8), ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial((num_tokens_including_cache + T.int64(63)) // T.int64(64), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("A_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(32), ax0)
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial((num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(A[T.int64(0), v0, v1, v2])
                                            T.writes(A_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            A_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache and v2 < num_tokens_including_cache, A[T.int64(0), v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("B_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(32), ax0)
                                            v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial((num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(64), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(B[T.int64(0), v0, v2, v1])
                                            T.writes(B_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            B_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v2 < num_tokens_including_cache, B[T.int64(0), v0, v2, v1], T.float16(0))
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("A_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(32), ax0)
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(4) * ((num_tokens_including_cache + T.int64(63)) // T.int64(64)), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(A_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(A_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A_1 = T.match_buffer(A_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(A_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A_1.data, A_1.elem_offset, A_1.strides[0] * T.int64(16), 1), A_1.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("B_reindex_pad_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(32), ax0)
                                        v1_o = T.axis.spatial(T.int64(8), ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(4) * ((num_tokens_including_cache + T.int64(63)) // T.int64(64)), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(B_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(B_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A_1 = T.match_buffer(B_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(B_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A_1.data, A_1.elem_offset, A_1.strides[0] * T.int64(16), 1), A_1.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(32), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial(T.int64(8), ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce((num_tokens_including_cache + T.int64(63)) // T.int64(64) * T.int64(4), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], A_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], B_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], A_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], B_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A_1 = T.match_buffer(A_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B_1 = T.match_buffer(B_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A_1.data, A_1.elem_offset // A_1.strides[0] // T.int64(16) * (A_1.strides[0] // T.int64(16)) + A_1.elem_offset % A_1.strides[0] // T.int64(16), B_1.data, B_1.elem_offset // B_1.strides[0] // T.int64(16) * (B_1.strides[0] // T.int64(16)) + B_1.elem_offset % B_1.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("matmul_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(32), ax0)
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(8), ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(matmul_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A_1 = T.match_buffer(matmul_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(matmul_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A_1.data, 16, 16, 16, A_1.elem_offset // A_1.strides[0] // T.int64(16) * (A_1.strides[0] // T.int64(16)) + A_1.elem_offset % A_1.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("matmul_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(32), ax0)
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(128), ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(matmul_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(matmul[T.int64(0), v0, v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache:
                                        matmul[T.int64(0), v0, v1, v2] = matmul_reindex_pad_shared_dyn[v0, v1, v2]
================================================================================
PrimFunc name: slice
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_slice: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache, vocab_size = T.int64(), T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_excluding_cache, vocab_size))
    slice = T.match_buffer(var_slice, (T.int64(1), T.int64(1), vocab_size))
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding((vocab_size + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("slice"):
                v0 = T.axis.spatial(vocab_size, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < vocab_size)
                T.reads(A[T.int64(0), num_tokens_excluding_cache - T.int64(1), v0])
                T.writes(slice[T.int64(0), T.int64(0), v0])
                slice[T.int64(0), T.int64(0), v0] = A[T.int64(0), num_tokens_excluding_cache - T.int64(1), v0]
================================================================================
PrimFunc name: fused_fused_decode5_fused_NT_matmul4_add
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv24: T.Buffer((T.int64(4096), T.int64(2752)), "uint32"), lv25: T.Buffer((T.int64(4096), T.int64(344)), "float16"), lv23: T.Buffer((T.int64(1), T.int64(1), T.int64(11008)), "float16"), lv19: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), T_add_intermediate_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    NT_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(32), T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    lv24_local = T.alloc_buffer((T.int64(4096), T.int64(2752)), "uint32", scope="local")
    lv23_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(11008)), "float16", scope="shared")
    for u_fused_ax0_fused_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
        for u_fused_ax0_fused_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2_0 in T.serial(T.int64(22), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                            for ax2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax2_3 in T.vectorized(T.int64(1)):
                                    with T.block("lv23_shared"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2 = T.axis.spatial(T.int64(11008), ax2_0 * T.int64(512) + ax2_1 * T.int64(32) + ax2_2 + ax2_3)
                                        T.where((ax2_0 * T.int64(16) + ax2_1) * T.int64(32) + ax2_2 + ax2_3 < T.int64(11008))
                                        T.reads(lv23[v0, v1, v2])
                                        T.writes(lv23_shared[v0, v1, v2])
                                        lv23_shared[v0, v1, v2] = lv23[v0, v1, v2]
                for u_fused_ax0_fused_fused_2_init in range(T.int64(1)):
                    for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init in T.vectorized(T.int64(4)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(128), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init)
                            v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = T.float16(0)
                for ax1_0_fused_ax1_1_fused_0 in T.serial(T.int64(86), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0_0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax0_1 in T.vectorized(T.int64(1)):
                            with T.block("lv24_local"):
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + ax0_0 + ax0_1)
                                v1 = T.axis.spatial(T.int64(2752), ax1_0_fused_ax1_1_fused_0 * T.int64(32) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 + ax1)
                                T.reads(lv24[v0, v1])
                                T.writes(lv24_local[v0, v1])
                                lv24_local[v0, v1] = lv24[v0, v1]
                    for u_fused_ax0_fused_fused_2, ax1_0_fused_ax1_1_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 in T.vectorized(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(128), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1)
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2)
                                vax1_0_fused_ax1_1_fused_2, vax1_0_fused_ax1_1_fused_0 = T.axis.remap("RR", [ax1_0_fused_ax1_1_fused_2, ax1_0_fused_ax1_1_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0], lv23_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused], lv24_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(32) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], lv25[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
                                T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] + lv23_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused] * ((T.Cast("float16", T.bitwise_and(T.shift_right(lv24_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(32) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], T.Cast("uint32", (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv25[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
        for ax2_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax2_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.spatial(T.int64(32), ax0)
                            v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = T.float16(0)
                        for ax1 in range(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 = T.axis.remap("SR", [ax0, ax1])
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                                T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0], NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0]
        for ax1_fused_1 in range(T.int64(1)):
            for ax1_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.reduce(T.int64(32), ax0)
                        v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax1_fused_0 + ax1_fused_1)
                        T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                        T.writes(NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                        with T.init():
                            NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = T.float16(0)
                        NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0]
        for ax0_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax0_fused_1 in range(T.int64(1)):
                with T.block("T_add"):
                    v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax0_fused_0 + ax0_fused_1)
                    T.reads(lv19[T.int64(0), T.int64(0), v0], NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                    T.writes(T_add_intermediate_intermediate[T.int64(0), T.int64(0), v0])
                    T_add_intermediate_intermediate[T.int64(0), T.int64(0), v0] = lv19[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0]
================================================================================
PrimFunc name: reshape
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(A: T.Buffer((T.int64(1), T.int64(1)), "int32"), T_reshape: T.Buffer((T.int64(1),), "int32")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(1))
                T.reads(A[T.int64(0), T.int64(0)])
                T.writes(T_reshape[T.int64(0)])
                T_reshape[T.int64(0)] = A[T.int64(0), T.int64(0)]
================================================================================
PrimFunc name: fused_fused_decode4_NT_matmul3
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv20: T.Buffer((T.int64(22016), T.int64(1024)), "uint32"), lv21: T.Buffer((T.int64(22016), T.int64(128)), "float16"), lv1654: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(22016)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1), T.int64(1), T.int64(22016)), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(64), T.int64(1), T.int64(1), T.int64(22016)), "float16", scope="local")
    lv20_local = T.alloc_buffer((T.int64(22016), T.int64(1024)), "uint32", scope="local")
    lv1654_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="shared")
    for u_fused_ax0_fused_fused_0 in T.thread_binding(T.int64(5504), thread="blockIdx.x"):
        for u_fused_ax0_fused_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
            for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2_0 in T.serial(T.int64(2), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                            for ax2_2 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax2_3 in T.vectorized(T.int64(8)):
                                    with T.block("lv1654_shared"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2 = T.axis.spatial(T.int64(4096), ax2_0 * T.int64(2048) + ax2_1 * T.int64(512) + ax2_2 * T.int64(8) + ax2_3)
                                        T.reads(lv1654[v0, v1, v2])
                                        T.writes(lv1654_shared[v0, v1, v2])
                                        lv1654_shared[v0, v1, v2] = lv1654[v0, v1, v2]
                for u_fused_ax0_fused_fused_2_init in range(T.int64(1)):
                    for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init in T.vectorized(T.int64(4)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init)
                            v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = T.float16(0)
                for ax1_0_fused_ax1_1_fused_0 in T.serial(T.int64(16), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0_0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax0_1 in T.vectorized(T.int64(1)):
                            with T.block("lv20_local"):
                                v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + ax0_0 + ax0_1)
                                v1 = T.axis.spatial(T.int64(1024), ax1_0_fused_ax1_1_fused_0 * T.int64(64) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 + ax1)
                                T.reads(lv20[v0, v1])
                                T.writes(lv20_local[v0, v1])
                                lv20_local[v0, v1] = lv20[v0, v1]
                    for u_fused_ax0_fused_fused_2, ax1_0_fused_ax1_1_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 in T.vectorized(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1)
                                v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2)
                                vax1_0_fused_ax1_1_fused_2, vax1_0_fused_ax1_1_fused_0 = T.axis.remap("RR", [ax1_0_fused_ax1_1_fused_2, ax1_0_fused_ax1_1_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0], lv1654_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused], lv20_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], lv21[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
                                T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] + lv1654_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused] * ((T.Cast("float16", T.bitwise_and(T.shift_right(lv20_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], T.Cast("uint32", (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv21[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
        for ax2_fused_0 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.spatial(T.int64(64), ax0)
                            v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = T.float16(0)
                        for ax1 in range(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 = T.axis.remap("SR", [ax0, ax1])
                                v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                                T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0], NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0]
        for ax1_fused_1 in range(T.int64(1)):
            for ax1_fused_0 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.reduce(T.int64(64), ax0)
                        v0 = T.axis.spatial(T.int64(22016), u_fused_ax0_fused_fused_0 * T.int64(4) + ax1_fused_0 + ax1_fused_1)
                        T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                        T.writes(NT_matmul_intermediate[T.int64(0), T.int64(0), v0])
                        with T.init():
                            NT_matmul_intermediate[T.int64(0), T.int64(0), v0] = T.float16(0)
                        NT_matmul_intermediate[T.int64(0), T.int64(0), v0] = NT_matmul_intermediate[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0]
================================================================================
PrimFunc name: fused_squeeze
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv1_2: T.Buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16"), T_squeeze_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(128)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_squeeze"):
                v0 = T.axis.spatial(T.int64(32), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(128))
                v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(128))
                T.reads(lv1_2[T.int64(0), T.int64(0), v0, v1])
                T.writes(T_squeeze_intermediate[T.int64(0), v0, v1])
                T_squeeze_intermediate[T.int64(0), v0, v1] = lv1_2[T.int64(0), T.int64(0), v0, v1]
================================================================================
PrimFunc name: split_rotary
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(fused_qkv_handle: T.handle, embedded_query_handle: T.handle, embedded_key_handle: T.handle, value_handle: T.handle, rotary_offset: T.int64):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    query_sequence_length = T.int64()
    Fused_QKV = T.match_buffer(fused_qkv_handle, (T.int64(1), query_sequence_length, T.int64(96), T.int64(128)), "float16")
    EmbeddedQuery = T.match_buffer(embedded_query_handle, (T.int64(1), query_sequence_length, T.int64(32), T.int64(128)), "float16")
    EmbeddedKey = T.match_buffer(embedded_key_handle, (T.int64(1), query_sequence_length, T.int64(32), T.int64(128)), "float16")
    Value = T.match_buffer(value_handle, (T.int64(1), query_sequence_length, T.int64(32), T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(query_sequence_length * T.int64(12), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("FusedRotaryEmbeddingAndSplitQKV"):
                v0 = T.axis.spatial(query_sequence_length, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(12288))
                v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(12288) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(Fused_QKV[T.int64(0), v0, v1, T.min(T.min(v2, T.int64(64) + v2), v2 - T.int64(64)):T.min(T.min(v2, T.int64(64) + v2), v2 - T.int64(64)) + T.int64(129)])
                T.writes(EmbeddedQuery[T.int64(0), v0, v1, v2], EmbeddedKey[T.int64(0), v0, v1 + T.int64(-32), v2], Value[T.int64(0), v0, v1 + T.int64(-64), v2])
                pos: T.float32 = T.Cast("float32", rotary_offset + v0 - query_sequence_length)
                inv_freq: T.float32 = T.float32(1) / T.pow(T.float32(10000), T.Cast("float32", v2 * T.int64(2) % T.int64(128)) / T.Cast("float32", T.int64(128)))
                freq: T.float32 = pos * inv_freq
                cos_value: T.float16 = T.Cast("float16", T.cos(freq))
                sin_value: T.float16 = T.Cast("float16", T.sin(freq))
                input_value: T.float16 = Fused_QKV[T.int64(0), v0, v1, v2]
                embedded_value: T.float16 = cos_value * input_value + sin_value * T.Select(v2 < T.int64(64), Fused_QKV[T.int64(0), v0, v1, v2 + T.int64(64)] * T.float16(-1), Fused_QKV[T.int64(0), v0, v1, v2 + T.int64(-64)])
                if v1 < T.int64(32):
                    EmbeddedQuery[T.int64(0), v0, v1, v2] = embedded_value
                else:
                    if v1 < T.int64(64):
                        EmbeddedKey[T.int64(0), v0, v1 + T.int64(-32), v2] = embedded_value
                    else:
                        Value[T.int64(0), v0, v1 + T.int64(-64), v2] = input_value
================================================================================
PrimFunc name: fused_transpose5
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv1_0: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv1_0 = T.match_buffer(p_lv1_0, (T.int64(1), num_tokens_excluding_cache, T.int64(32), T.int64(128)), "float16")
    T_transpose_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), num_tokens_excluding_cache, T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_transpose"):
                v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // (T.int64(128) * num_tokens_excluding_cache))
                v1 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(128) * num_tokens_excluding_cache) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(lv1_0[T.int64(0), v1, v0, v2])
                T.writes(T_transpose_intermediate[T.int64(0), v0, v1, v2])
                T_transpose_intermediate[T.int64(0), v0, v1, v2] = lv1_0[T.int64(0), v1, v0, v2]
================================================================================
PrimFunc name: fused_reshape2_split_rotary
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv: T.Buffer((T.int64(1), T.int64(1), T.int64(12288)), "float16"), EmbeddedQuery_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16"), EmbeddedKey_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16"), Value_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16"), num_tokens_including_cache: T.int64):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(T.int64(12), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("FusedRotaryEmbeddingAndSplitQKV"):
                v0 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(128))
                v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(128))
                T.reads(lv[T.int64(0), T.int64(0), v0 * T.int64(128) + v1 - T.int64(64):v0 * T.int64(128) + v1 - T.int64(64) + T.int64(129)])
                T.writes(EmbeddedQuery_intermediate[T.int64(0), T.int64(0), v0, v1], EmbeddedKey_intermediate[T.int64(0), T.int64(0), v0 + T.int64(-32), v1], Value_intermediate[T.int64(0), T.int64(0), v0 + T.int64(-64), v1])
                pos: T.float32 = T.Cast("float32", T.Add(num_tokens_including_cache, T.int64(0)) - T.int64(1))
                inv_freq: T.float32 = T.float32(1) / T.pow(T.float32(10000), T.Cast("float32", v1 * T.int64(2) % T.int64(128)) / T.Cast("float32", T.int64(128)))
                freq: T.float32 = pos * inv_freq
                cos_value: T.float16 = T.Cast("float16", T.cos(freq))
                sin_value: T.float16 = T.Cast("float16", T.sin(freq))
                input_value: T.float16 = lv[T.int64(0), T.int64(0), v0 * T.int64(128) + v1]
                embedded_value: T.float16 = cos_value * input_value + sin_value * T.Select(v1 < T.int64(64), lv[T.int64(0), T.int64(0), v0 * T.int64(128) + (v1 + T.int64(64))] * T.float16(-1), lv[T.int64(0), T.int64(0), v0 * T.int64(128) + (v1 + T.int64(-64))])
                if v0 < T.int64(32):
                    EmbeddedQuery_intermediate[T.int64(0), T.int64(0), v0, v1] = embedded_value
                else:
                    if v0 < T.int64(64):
                        EmbeddedKey_intermediate[T.int64(0), T.int64(0), v0 + T.int64(-32), v1] = embedded_value
                    else:
                        Value_intermediate[T.int64(0), T.int64(0), v0 + T.int64(-64), v1] = input_value
================================================================================
PrimFunc name: fused_squeeze1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv1_2: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv1_2 = T.match_buffer(p_lv1_2, (T.int64(1), num_tokens_excluding_cache, T.int64(32), T.int64(128)), "float16")
    T_squeeze_intermediate = T.match_buffer(p_output0, (num_tokens_excluding_cache, T.int64(32), T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_squeeze"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                v1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(lv1_2[T.int64(0), v0, v1, v2])
                T.writes(T_squeeze_intermediate[v0, v1, v2])
                T_squeeze_intermediate[v0, v1, v2] = lv1_2[T.int64(0), v0, v1, v2]
================================================================================
PrimFunc name: reshape1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(A: T.Buffer((T.int64(1), T.int64(4096)), "float16"), T_reshape: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(T.int64(4096), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.reads(A[T.int64(0), v0])
                T.writes(T_reshape[T.int64(0), T.int64(0), v0])
                T_reshape[T.int64(0), T.int64(0), v0] = A[T.int64(0), v0]
================================================================================
PrimFunc name: fused_transpose6_reshape4
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv1648: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16"), T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(T.int64(4096), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.reads(lv1648[T.int64(0), v0 // T.int64(128), T.int64(0), v0 % T.int64(128)])
                T.writes(T_reshape_intermediate[T.int64(0), T.int64(0), v0])
                T_reshape_intermediate[T.int64(0), T.int64(0), v0] = lv1648[T.int64(0), v0 // T.int64(128), T.int64(0), v0 % T.int64(128)]
================================================================================
PrimFunc name: fused_split1_silu1_multiply1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv164: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv164 = T.match_buffer(p_lv164, (T.int64(1), num_tokens_excluding_cache, T.int64(22016)), "float16")
    T_multiply_intermediate_1 = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, T.int64(11008)), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding((num_tokens_excluding_cache * T.int64(11008) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_multiply_1"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(11008))
                v1 = T.axis.spatial(T.int64(11008), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(11008))
                T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < num_tokens_excluding_cache * T.int64(11008))
                T.reads(lv164[T.int64(0), v0, v1:v1 + T.int64(11009)])
                T.writes(T_multiply_intermediate_1[T.int64(0), v0, v1])
                T_multiply_intermediate_1[T.int64(0), v0, v1] = lv164[T.int64(0), v0, v1] * T.sigmoid(lv164[T.int64(0), v0, v1]) * lv164[T.int64(0), v0, v1 + T.int64(11008)]
================================================================================
PrimFunc name: fused_softmax2_cast4
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv36: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache, num_tokens_including_cache = T.int64(), T.int64()
    lv36 = T.match_buffer(p_lv36, (T.int64(1), T.int64(32), num_tokens_excluding_cache, num_tokens_including_cache))
    compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), num_tokens_excluding_cache, num_tokens_including_cache), "float16")
    # with T.block("root"):
    T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), num_tokens_excluding_cache), scope="shared")
    T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), num_tokens_excluding_cache), scope="shared")
    for ax0_ax1_fused in T.thread_binding(num_tokens_excluding_cache * T.int64(32), thread="blockIdx.x"):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
            for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_fused_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_maxelem"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // num_tokens_excluding_cache + ax0)
                        v1 = T.axis.spatial(num_tokens_excluding_cache, ax0_ax1_fused % num_tokens_excluding_cache + ax1)
                        v2 = T.axis.reduce(num_tokens_including_cache, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                        T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < num_tokens_including_cache)
                        T.reads(lv36[T.int64(0), v0, v1, v2])
                        T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                        with T.init():
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv36[T.int64(0), v0, v1, v2])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
            for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_fused_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_expsum"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // num_tokens_excluding_cache + ax0)
                        v1 = T.axis.spatial(num_tokens_excluding_cache, ax0_ax1_fused % num_tokens_excluding_cache + ax1)
                        v2 = T.axis.reduce(num_tokens_including_cache, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                        T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < num_tokens_including_cache)
                        T.reads(lv36[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                        T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                        with T.init():
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                        T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv36[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
        for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
            for ax2_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("compute"):
                    v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // num_tokens_excluding_cache)
                    v1 = T.axis.spatial(num_tokens_excluding_cache, ax0_ax1_fused % num_tokens_excluding_cache)
                    v2 = T.axis.spatial(num_tokens_including_cache, ax2_0 * T.int64(256) + ax2_1)
                    T.where(ax2_0 * T.int64(256) + ax2_1 < num_tokens_including_cache)
                    T.reads(lv36[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                    T.writes(compute_intermediate[T.int64(0), v0, v1, v2])
                    compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv36[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
================================================================================
PrimFunc name: fused_fused_decode2_NT_matmul6
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv811: T.Buffer((T.int64(12288), T.int64(1024)), "uint32"), lv812: T.Buffer((T.int64(12288), T.int64(128)), "float16"), p_lv6: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv6 = T.match_buffer(p_lv6, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, T.int64(12288)), "float16")
    # with T.block("root"):
    lv6_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    decode_intermediate_intermediate_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(12288), T.int64(4096)), "float16", scope="shared.dyn")
    lv6_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.matrix_a")
    decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(12288), T.int64(4096)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(12288)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(12288)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(1), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(96), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(1), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial(T.int64(768), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(64), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv6_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv6[v0, v1, v2])
                                            T.writes(lv6_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv6_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv6[v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("decode_intermediate_intermediate_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(T.int64(12288), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv811[v1, v2 // T.int64(4)], lv812[v1, v2 // T.int64(32)])
                                            T.writes(decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv811[v1, v2 // T.int64(4)], T.Cast("uint32", v2 % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv812[v1, v2 // T.int64(32)]
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv6_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv6_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv6_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv6_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv6_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("decode_intermediate_intermediate_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(768), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(1), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial(T.int64(768), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv6_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv6_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv6_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(768), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(12288), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(NT_matmul_intermediate[T.int64(0), v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache:
                                        NT_matmul_intermediate[T.int64(0), v1, v2] = NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2]
================================================================================
PrimFunc name: fused_fused_decode3_fused_NT_matmul2_add
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv16: T.Buffer((T.int64(4096), T.int64(1024)), "uint32"), lv17: T.Buffer((T.int64(4096), T.int64(128)), "float16"), lv15: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), lv1613: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), T_add_intermediate_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    NT_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(32), T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="local")
    lv16_local = T.alloc_buffer((T.int64(4096), T.int64(1024)), "uint32", scope="local")
    lv15_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="shared")
    for u_fused_ax0_fused_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
        for u_fused_ax0_fused_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2_0 in T.serial(T.int64(1), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                            for ax2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax2_3 in T.vectorized(T.int64(8)):
                                    with T.block("lv15_shared"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2 = T.axis.spatial(T.int64(4096), ax2_0 * T.int64(4096) + ax2_1 * T.int64(256) + ax2_2 * T.int64(8) + ax2_3)
                                        T.reads(lv15[v0, v1, v2])
                                        T.writes(lv15_shared[v0, v1, v2])
                                        lv15_shared[v0, v1, v2] = lv15[v0, v1, v2]
                for u_fused_ax0_fused_fused_2_init in range(T.int64(1)):
                    for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init in T.vectorized(T.int64(4)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(128), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init)
                            v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = T.float16(0)
                for ax1_0_fused_ax1_1_fused_0 in T.serial(T.int64(32), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0_0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax0_1 in T.vectorized(T.int64(1)):
                            with T.block("lv16_local"):
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + ax0_0 + ax0_1)
                                v1 = T.axis.spatial(T.int64(1024), ax1_0_fused_ax1_1_fused_0 * T.int64(32) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 + ax1)
                                T.reads(lv16[v0, v1])
                                T.writes(lv16_local[v0, v1])
                                lv16_local[v0, v1] = lv16[v0, v1]
                    for u_fused_ax0_fused_fused_2, ax1_0_fused_ax1_1_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 in T.vectorized(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(128), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1)
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2)
                                vax1_0_fused_ax1_1_fused_2, vax1_0_fused_ax1_1_fused_0 = T.axis.remap("RR", [ax1_0_fused_ax1_1_fused_2, ax1_0_fused_ax1_1_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0], lv15_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused], lv16_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(32) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], lv17[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
                                T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] + lv15_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused] * ((T.Cast("float16", T.bitwise_and(T.shift_right(lv16_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(32) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], T.Cast("uint32", (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv17[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(128) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
        for ax2_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax2_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.spatial(T.int64(32), ax0)
                            v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = T.float16(0)
                        for ax1 in range(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 = T.axis.remap("SR", [ax0, ax1])
                                v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                                T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0], NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0]
        for ax1_fused_1 in range(T.int64(1)):
            for ax1_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.reduce(T.int64(32), ax0)
                        v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax1_fused_0 + ax1_fused_1)
                        T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                        T.writes(NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                        with T.init():
                            NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = T.float16(0)
                        NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0]
        for ax0_fused_0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
            for ax0_fused_1 in range(T.int64(1)):
                with T.block("T_add"):
                    v0 = T.axis.spatial(T.int64(4096), u_fused_ax0_fused_fused_0 * T.int64(16) + ax0_fused_0 + ax0_fused_1)
                    T.reads(lv1613[T.int64(0), T.int64(0), v0], NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0])
                    T.writes(T_add_intermediate_intermediate[T.int64(0), T.int64(0), v0])
                    T_add_intermediate_intermediate[T.int64(0), T.int64(0), v0] = lv1613[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_local[T.int64(0), T.int64(0), v0]
================================================================================
PrimFunc name: matmul6
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_B: T.handle, matmul: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache), "float16")
    B = T.match_buffer(var_B, (T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16")
    # with T.block("root"):
    matmul_rf_local = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16", scope="local")
    for ax0_ax1_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
            for ax2_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                with T.block("matmul_rf_init"):
                    vax2_fused_1 = T.axis.spatial(T.int64(16), ax2_fused_1)
                    v0 = T.axis.spatial(T.int64(32), (ax0_ax1_fused_0 * T.int64(16) + ax0_ax1_fused_1) // T.int64(128))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(16) + ax0_ax1_fused_1) % T.int64(128))
                    T.reads()
                    T.writes(matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1])
                    matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1] = T.float16(0)
                for ax2_fused_0, u in T.grid((num_tokens_including_cache + T.int64(15)) // T.int64(16), 1):
                    with T.block("matmul_rf_update"):
                        vax2_fused_1 = T.axis.spatial(T.int64(16), ax2_fused_1)
                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_fused_0 * T.int64(16) + ax0_ax1_fused_1) // T.int64(128))
                        v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(16) + ax0_ax1_fused_1) % T.int64(128))
                        vax2_fused_0 = T.axis.reduce((num_tokens_including_cache + T.int64(15)) // T.int64(16), ax2_fused_0)
                        T.where(ax2_fused_0 * T.int64(16) + ax2_fused_1 < num_tokens_including_cache)
                        T.reads(matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1], A[T.int64(0), v0, T.int64(0), vax2_fused_0 * T.int64(16) + vax2_fused_1], B[T.int64(0), v0, vax2_fused_0 * T.int64(16) + vax2_fused_1, v1])
                        T.writes(matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1])
                        matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1] = matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1] + A[T.int64(0), v0, T.int64(0), vax2_fused_0 * T.int64(16) + vax2_fused_1] * B[T.int64(0), v0, vax2_fused_0 * T.int64(16) + vax2_fused_1, v1]
        for ax1_ax2_fused in T.thread_binding(T.int64(16), thread="threadIdx.x"):
            for ax0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                with T.block("matmul"):
                    vax2_fused_1 = T.axis.reduce(T.int64(16), ax0)
                    v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused_0 // T.int64(8))
                    v1 = T.axis.spatial(T.int64(128), ax0_ax1_fused_0 % T.int64(8) * T.int64(16) + ax1_ax2_fused)
                    T.reads(matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1])
                    T.writes(matmul[T.int64(0), v0, T.int64(0), v1])
                    with T.init():
                        matmul[T.int64(0), v0, T.int64(0), v1] = T.float16(0)
                    matmul[T.int64(0), v0, T.int64(0), v1] = matmul[T.int64(0), v0, T.int64(0), v1] + matmul_rf_local[vax2_fused_1, T.int64(0), v0, T.int64(0), v1]
================================================================================
PrimFunc name: fused_min_max_triu_te_broadcast_to
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_output0: T.handle, num_tokens_excluding_cache: T.int64):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    T_broadcast_to_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1), num_tokens_excluding_cache, num_tokens_excluding_cache), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding((num_tokens_excluding_cache * num_tokens_excluding_cache + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_broadcast_to"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // num_tokens_excluding_cache)
                v1 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % num_tokens_excluding_cache)
                T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < num_tokens_excluding_cache * num_tokens_excluding_cache)
                T.reads()
                T.writes(T_broadcast_to_intermediate[T.int64(0), T.int64(0), v0, v1])
                T_broadcast_to_intermediate[T.int64(0), T.int64(0), v0, v1] = T.Select(v0 < v1, T.float16(-65504), T.float16(65504))
================================================================================
PrimFunc name: extend_te
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_concat_te: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(1), num_tokens_excluding_cache, num_tokens_excluding_cache), "float16")
    num_tokens_including_cache = T.int64()
    concat_te = T.match_buffer(var_concat_te, (T.int64(1), T.int64(1), num_tokens_excluding_cache, num_tokens_including_cache), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding((num_tokens_excluding_cache * num_tokens_including_cache + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("concat_te"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % (num_tokens_including_cache * num_tokens_excluding_cache) // num_tokens_including_cache)
                v1 = T.axis.spatial(num_tokens_including_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % num_tokens_including_cache)
                T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < num_tokens_excluding_cache * num_tokens_including_cache)
                T.reads(A[T.int64(0), T.int64(0), v0, v1 + (num_tokens_excluding_cache - num_tokens_including_cache)])
                T.writes(concat_te[T.int64(0), T.int64(0), v0, v1])
                concat_te[T.int64(0), T.int64(0), v0, v1] = T.if_then_else(v1 < num_tokens_including_cache - num_tokens_excluding_cache, T.float16(65504), A[T.int64(0), T.int64(0), v0, v1 + (num_tokens_excluding_cache - num_tokens_including_cache)])
================================================================================
PrimFunc name: fused_fused_decode1_take
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv: T.handle, p_lv1: T.handle, lv1611: T.Buffer((1,), "int32"), T_take_intermediate: T.Buffer((1, 4096), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int32()
    lv = T.match_buffer(p_lv, (vocab_size, 1024), "uint32")
    lv1 = T.match_buffer(p_lv1, (vocab_size, 128), "float16")
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding(4, thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
            with T.block("T_take"):
                v0 = T.axis.spatial(4096, ax0_fused_0 * 1024 + ax0_fused_1)
                T.reads(lv1611[0], lv[lv1611[0], v0 // 4], lv1[lv1611[0], v0 // 32])
                T.writes(T_take_intermediate[0, v0])
                T_take_intermediate[0, v0] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv[lv1611[0], v0 // 4], T.Cast("uint32", v0 % 4) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv1[lv1611[0], v0 // 32]
================================================================================
PrimFunc name: fused_NT_matmul1_divide1_maximum_minimum_cast
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv1637: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16"), p_lv1638: T.handle, p_lv1614: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    lv1638 = T.match_buffer(p_lv1638, (T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16")
    lv1614 = T.match_buffer(p_lv1614, (T.int64(1), T.int64(1), T.int64(1), num_tokens_including_cache), "float16")
    compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache))
    # with T.block("root"):
    NT_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache), "float16", scope="local")
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(64), T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache), "float16", scope="local")
    lv1638_local = T.alloc_buffer((T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16", scope="local")
    lv1637_shared = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16", scope="shared")
    for ax0_fused_ax1_fused_fused_0 in T.thread_binding(num_tokens_including_cache * T.int64(32), thread="blockIdx.x"):
        for ax0_fused_ax1_fused_fused_1 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                    for ax3_0 in T.serial(T.int64(1), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax3_1 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
                            for ax3_2 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax3_3 in T.vectorized(T.int64(2)):
                                    with T.block("lv1637_shared"):
                                        v0 = T.axis.spatial(T.int64(1), ax0)
                                        v1 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache + ax1)
                                        v2 = T.axis.spatial(T.int64(1), ax2)
                                        v3 = T.axis.spatial(T.int64(128), ax3_0 * T.int64(128) + ax3_1 * T.int64(128) + ax3_2 * T.int64(2) + ax3_3)
                                        T.reads(lv1637[v0, v1, v2, v3])
                                        T.writes(lv1637_shared[v0, v1, v2, v3])
                                        lv1637_shared[v0, v1, v2, v3] = lv1637[v0, v1, v2, v3]
                for ax0_fused_ax1_fused_fused_2_init in range(T.int64(1)):
                    for ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1_init in T.vectorized(T.int64(2)):
                        with T.block("NT_matmul_rf_init"):
                            vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused = T.axis.spatial(T.int64(128), ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 * T.int64(2) + ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1_init)
                            v0 = T.axis.spatial(T.int64(32), (ax0_fused_ax1_fused_fused_0 + ax0_fused_ax1_fused_fused_1 + ax0_fused_ax1_fused_fused_2_init) // num_tokens_including_cache)
                            v1 = T.axis.spatial(num_tokens_including_cache, (ax0_fused_ax1_fused_fused_0 + ax0_fused_ax1_fused_fused_1 + ax0_fused_ax1_fused_fused_2_init) % num_tokens_including_cache)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1])
                            NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1] = T.float16(0)
                for ax2_fused_u_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0, ax1, ax2_0, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        for ax2_1 in T.vectorized(T.int64(1)):
                            with T.block("lv1638_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache + ax1)
                                v2 = T.axis.spatial(num_tokens_including_cache, ax0_fused_ax1_fused_fused_0 % num_tokens_including_cache + ax2_0 + ax2_1)
                                v3 = T.axis.spatial(T.int64(128), ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 * T.int64(2) + ax3)
                                T.reads(lv1638[v0, v1, v2, v3])
                                T.writes(lv1638_local[v0, v1, v2, v3])
                                lv1638_local[v0, v1, v2, v3] = lv1638[v0, v1, v2, v3]
                    for ax0_fused_ax1_fused_fused_2, ax2_fused_u_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1 in T.vectorized(T.int64(2)):
                            with T.block("NT_matmul_rf_update"):
                                vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused = T.axis.spatial(T.int64(128), ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 * T.int64(2) + ax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1)
                                v0 = T.axis.spatial(T.int64(32), (ax0_fused_ax1_fused_fused_0 + ax0_fused_ax1_fused_fused_1 + ax0_fused_ax1_fused_fused_2) // num_tokens_including_cache)
                                v1 = T.axis.spatial(num_tokens_including_cache, (ax0_fused_ax1_fused_fused_0 + ax0_fused_ax1_fused_fused_1 + ax0_fused_ax1_fused_fused_2) % num_tokens_including_cache)
                                vax2_fused_u_fused_2, vax2_fused_u_fused_0 = T.axis.remap("RR", [ax2_fused_u_fused_2, ax2_fused_u_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1], lv1637_shared[T.int64(0), v0, T.int64(0), vax2_fused_u_fused_0 * T.int64(128) + vax2_fused_u_fused_2 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused], lv1638_local[T.int64(0), v0, v1, vax2_fused_u_fused_0 * T.int64(128) + vax2_fused_u_fused_2 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused])
                                T.writes(NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1])
                                NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1] = NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused, T.int64(0), v0, T.int64(0), v1] + lv1637_shared[T.int64(0), v0, T.int64(0), vax2_fused_u_fused_0 * T.int64(128) + vax2_fused_u_fused_2 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused] * lv1638_local[T.int64(0), v0, v1, vax2_fused_u_fused_0 * T.int64(128) + vax2_fused_u_fused_2 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused]
        for ax2_ax3_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_ax3_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_ax3_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 = T.axis.spatial(T.int64(64), ax0)
                            v0 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache)
                            v1 = T.axis.spatial(num_tokens_including_cache, ax0_fused_ax1_fused_fused_0 % num_tokens_including_cache)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1])
                            NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1] = T.float16(0)
                        for ax1 in range(T.int64(2)):
                            with T.block("NT_matmul_rf_update"):
                                vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1 = T.axis.remap("SR", [ax0, ax1])
                                v0 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache)
                                v1 = T.axis.spatial(num_tokens_including_cache, ax0_fused_ax1_fused_fused_0 % num_tokens_including_cache)
                                T.reads(NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1], NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1, T.int64(0), v0, T.int64(0), v1])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1])
                                NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1] = NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1] + NT_matmul_intermediate_rf_local[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 * T.int64(2) + vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_1, T.int64(0), v0, T.int64(0), v1]
        for ax1_ax2_fused_1 in range(T.int64(1)):
            for ax1_ax2_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0 = T.axis.reduce(T.int64(64), ax0)
                        v0 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache)
                        v1 = T.axis.spatial(num_tokens_including_cache, ax0_fused_ax1_fused_fused_0 % num_tokens_including_cache)
                        T.reads(NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1])
                        T.writes(NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1])
                        with T.init():
                            NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1] = T.float16(0)
                        NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1] = NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1] + NT_matmul_intermediate_rf_local_1[vax2_fused_u_fused_1_ax2_fused_u_fused_3_fused_0, T.int64(0), v0, T.int64(0), v1]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread="threadIdx.y"):
            for ax0_ax1_fused_1 in range(T.int64(1)):
                with T.block("compute"):
                    v0 = T.axis.spatial(T.int64(32), ax0_fused_ax1_fused_fused_0 // num_tokens_including_cache)
                    v1 = T.axis.spatial(num_tokens_including_cache, ax0_fused_ax1_fused_fused_0 % num_tokens_including_cache)
                    T.reads(NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1], lv1614[T.int64(0), T.int64(0), T.int64(0), v1])
                    T.writes(compute_intermediate[T.int64(0), v0, T.int64(0), v1])
                    compute_intermediate[T.int64(0), v0, T.int64(0), v1] = T.Cast("float32", T.min(T.max(NT_matmul_intermediate_local[T.int64(0), v0, T.int64(0), v1] * T.float16(0.088397790055248615), T.float16(-65504)), lv1614[T.int64(0), T.int64(0), T.int64(0), v1]))
================================================================================
PrimFunc name: fused_fused_decode1_take1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv807: T.handle, p_lv808: T.handle, p_lv: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int32()
    lv807 = T.match_buffer(p_lv807, (vocab_size, 1024), "uint32")
    lv808 = T.match_buffer(p_lv808, (vocab_size, 128), "float16")
    num_tokens_excluding_cache = T.int32()
    lv = T.match_buffer(p_lv, (num_tokens_excluding_cache,), "int32")
    T_take_intermediate = T.match_buffer(p_output0, (num_tokens_excluding_cache, 4096), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(num_tokens_excluding_cache * 4, thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
            with T.block("T_take"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * 1024 + ax0_ax1_fused_1) // 4096)
                v1 = T.axis.spatial(4096, (ax0_ax1_fused_0 * 1024 + ax0_ax1_fused_1) % 4096)
                T.reads(lv[v0], lv807[lv[v0], v1 // 4], lv808[lv[v0], v1 // 32])
                T.writes(T_take_intermediate[v0, v1])
                T_take_intermediate[v0, v1] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv807[lv[v0], v1 // 4], T.Cast("uint32", v1 % 4) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv808[lv[v0], v1 // 32]
================================================================================
PrimFunc name: fused_fused_decode4_NT_matmul9
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv826: T.Buffer((T.int64(22016), T.int64(1024)), "uint32"), lv827: T.Buffer((T.int64(22016), T.int64(128)), "float16"), p_lv45: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv45 = T.match_buffer(p_lv45, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, T.int64(22016)), "float16")
    # with T.block("root"):
    lv45_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    decode_intermediate_intermediate_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(22016), T.int64(4096)), "float16", scope="shared.dyn")
    lv45_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.matrix_a")
    decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(22016), T.int64(4096)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(22016)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(22016)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(1), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(172), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(1), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial(T.int64(1376), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(64), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv45_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv45[v0, v1, v2])
                                            T.writes(lv45_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv45_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv45[v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("decode_intermediate_intermediate_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(T.int64(22016), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv826[v1, v2 // T.int64(4)], lv827[v1, v2 // T.int64(32)])
                                            T.writes(decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv826[v1, v2 // T.int64(4)], T.Cast("uint32", v2 % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv827[v1, v2 // T.int64(32)]
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv45_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv45_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv45_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv45_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv45_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("decode_intermediate_intermediate_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(1376), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(1), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial(T.int64(1376), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv45_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv45_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv45_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(1376), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(22016), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(NT_matmul_intermediate[T.int64(0), v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache:
                                        NT_matmul_intermediate[T.int64(0), v1, v2] = NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2]
================================================================================
PrimFunc name: reshape5
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_reshape: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_excluding_cache), "int32")
    T_reshape = T.match_buffer(var_T_reshape, (num_tokens_excluding_cache,), "int32")
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding((num_tokens_excluding_cache + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < num_tokens_excluding_cache)
                T.reads(A[T.int64(0), v0])
                T.writes(T_reshape[v0])
                T_reshape[v0] = A[T.int64(0), v0]
================================================================================
PrimFunc name: reshape3
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_reshape: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    A = T.match_buffer(var_A, (num_tokens_including_cache, T.int64(32), T.int64(128)), "float16")
    T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), num_tokens_including_cache, T.int64(32), T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_including_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(num_tokens_including_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                v1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(A[v0, v1, v2])
                T.writes(T_reshape[T.int64(0), v0, v1, v2])
                T_reshape[T.int64(0), v0, v1, v2] = A[v0, v1, v2]
================================================================================
PrimFunc name: transpose9
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_transpose: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(32), num_tokens_excluding_cache, T.int64(128)), "float16")
    T_transpose = T.match_buffer(var_T_transpose, (T.int64(1), num_tokens_excluding_cache, T.int64(32), T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_transpose"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                v1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(A[T.int64(0), v1, v0, v2])
                T.writes(T_transpose[T.int64(0), v0, v1, v2])
                T_transpose[T.int64(0), v0, v1, v2] = A[T.int64(0), v1, v0, v2]
================================================================================
PrimFunc name: rms_norm1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), B: T.Buffer((T.int64(4096),), "float16"), rms_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    Ared_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1)), scope="shared")
    Ared_temp_rf_local = T.alloc_buffer((T.int64(1024), T.int64(1), T.int64(1)), scope="local")
    for ax0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
        for ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
            with T.block("Ared_temp_rf_init"):
                vax1_fused_1 = T.axis.spatial(T.int64(1024), ax1_fused_1)
                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                T.reads()
                T.writes(Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)])
                Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)] = T.float32(0)
            for ax1_fused_0, u in T.grid(T.int64(4), 1):
                with T.block("Ared_temp_rf_update"):
                    vax1_fused_1 = T.axis.spatial(T.int64(1024), ax1_fused_1)
                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                    vax1_fused_0 = T.axis.reduce(T.int64(4), ax1_fused_0)
                    T.reads(Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)], A[T.int64(0), T.int64(0), vax1_fused_0 * T.int64(1024) + vax1_fused_1])
                    T.writes(Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)])
                    Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)] = Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)] + T.Cast("float32", A[T.int64(0), T.int64(0), vax1_fused_0 * T.int64(1024) + vax1_fused_1]) * T.Cast("float32", A[T.int64(0), T.int64(0), vax1_fused_0 * T.int64(1024) + vax1_fused_1])
        for ax1_fused in range(T.int64(1)):
            for ax0 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("Ared_temp"):
                    vax1_fused_1 = T.axis.reduce(T.int64(1024), ax0)
                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)])
                    T.writes(Ared_temp_shared[T.int64(0), T.int64(0)])
                    with T.init():
                        Ared_temp_shared[T.int64(0), T.int64(0)] = T.float32(0)
                    Ared_temp_shared[T.int64(0), T.int64(0)] = Ared_temp_shared[T.int64(0), T.int64(0)] + Ared_temp_rf_local[vax1_fused_1, T.int64(0), T.int64(0)]
        for ax0_fused_0 in range(T.int64(4)):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("rms_norm"):
                    v0 = T.axis.spatial(T.int64(4096), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(B[v0], A[T.int64(0), T.int64(0), v0], Ared_temp_shared[T.int64(0), T.int64(0)])
                    T.writes(rms_norm[T.int64(0), T.int64(0), v0])
                    rms_norm[T.int64(0), T.int64(0), v0] = T.Cast("float16", T.Cast("float32", B[v0]) * (T.Cast("float32", A[T.int64(0), T.int64(0), v0]) / T.sqrt(Ared_temp_shared[T.int64(0), T.int64(0)] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05))))
================================================================================
PrimFunc name: reshape6
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_reshape: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (num_tokens_excluding_cache, T.int64(4096)), "float16")
    T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(4096))
                v1 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(4096))
                T.reads(A[v0, v1])
                T.writes(T_reshape[T.int64(0), v0, v1])
                T_reshape[T.int64(0), v0, v1] = A[v0, v1]
================================================================================
PrimFunc name: fused_fused_decode5_fused_NT_matmul10_add1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv830: T.Buffer((T.int64(4096), T.int64(2752)), "uint32"), lv831: T.Buffer((T.int64(4096), T.int64(344)), "float16"), p_lv829: T.handle, p_lv825: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv829 = T.match_buffer(p_lv829, (T.int64(1), num_tokens_excluding_cache, T.int64(11008)), "float16")
    lv825 = T.match_buffer(p_lv825, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    T_add_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    # with T.block("root"):
    lv829_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(11008)), "float16", scope="shared.dyn")
    decode_intermediate_intermediate_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(11008)), "float16", scope="shared.dyn")
    lv829_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(11008)), "float16", scope="wmma.matrix_a")
    decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(11008)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(1), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(1), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(172), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv829_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(11008), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv829[v0, v1, v2])
                                            T.writes(lv829_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv829_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv829[v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("decode_intermediate_intermediate_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(T.int64(4096), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(11008), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv830[v1, v2 // T.int64(4)], lv831[v1, v2 // T.int64(32)])
                                            T.writes(decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv830[v1, v2 // T.int64(4)], T.Cast("uint32", v2 % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv831[v1, v2 // T.int64(32)]
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv829_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(688), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv829_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv829_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv829_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv829_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("decode_intermediate_intermediate_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(688), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(1), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(688), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv829_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv829_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv829_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(4096), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(lv825[T.int64(0), v1, v2], NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(T_add_intermediate_intermediate[T.int64(0), v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache:
                                        T_add_intermediate_intermediate[T.int64(0), v1, v2] = lv825[T.int64(0), v1, v2] + NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2]
================================================================================
PrimFunc name: fused_transpose4
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv1_0: T.Buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16"), T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(128)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_transpose"):
                v0 = T.axis.spatial(T.int64(32), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(128))
                v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(128))
                T.reads(lv1_0[T.int64(0), T.int64(0), v0, v1])
                T.writes(T_transpose_intermediate[T.int64(0), v0, T.int64(0), v1])
                T_transpose_intermediate[T.int64(0), v0, T.int64(0), v1] = lv1_0[T.int64(0), T.int64(0), v0, v1]
================================================================================
PrimFunc name: fused_fused_decode3_fused_NT_matmul8_add1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv822: T.Buffer((T.int64(4096), T.int64(1024)), "uint32"), lv823: T.Buffer((T.int64(4096), T.int64(128)), "float16"), p_lv41: T.handle, p_lv2: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv41 = T.match_buffer(p_lv41, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    lv2 = T.match_buffer(p_lv2, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    T_add_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    # with T.block("root"):
    lv41_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    decode_intermediate_intermediate_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float16", scope="shared.dyn")
    lv41_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.matrix_a")
    decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(1), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(1), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(64), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv41_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv41[v0, v1, v2])
                                            T.writes(lv41_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv41_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv41[v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("decode_intermediate_intermediate_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(T.int64(4096), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv822[v1, v2 // T.int64(4)], lv823[v1, v2 // T.int64(32)])
                                            T.writes(decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            decode_intermediate_intermediate_reindex_shared_dyn[v0, v1, v2] = (T.Cast("float16", T.bitwise_and(T.shift_right(lv822[v1, v2 // T.int64(4)], T.Cast("uint32", v2 % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv823[v1, v2 // T.int64(32)]
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv41_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv41_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv41_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv41_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv41_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("decode_intermediate_intermediate_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(1), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv41_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv41_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv41_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(decode_intermediate_intermediate_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(256), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(4096), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(lv2[T.int64(0), v1, v2], NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(T_add_intermediate_intermediate[T.int64(0), v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache:
                                        T_add_intermediate_intermediate[T.int64(0), v1, v2] = lv2[T.int64(0), v1, v2] + NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2]
================================================================================
PrimFunc name: full
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_T_full: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    T_full = T.match_buffer(var_T_full, (T.int64(1), T.int64(1), T.int64(1), num_tokens_including_cache), "float16")
    # with T.block("root"):
    for ax0_fused_0 in T.thread_binding((num_tokens_including_cache + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
        for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_full"):
                v0 = T.axis.spatial(num_tokens_including_cache, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < num_tokens_including_cache)
                T.reads()
                T.writes(T_full[T.int64(0), T.int64(0), T.int64(0), v0])
                T_full[T.int64(0), T.int64(0), T.int64(0), v0] = T.float16(65504)
================================================================================
PrimFunc name: fused_softmax1_cast1
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv1645: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    lv1645 = T.match_buffer(p_lv1645, (T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache))
    compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), T.int64(1), num_tokens_including_cache), "float16")
    # with T.block("root"):
    T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(1)), scope="shared")
    T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(1)), scope="shared")
    for ax0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
        for ax0 in range(T.int64(1)):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax1_fused_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_maxelem"):
                        v0 = T.axis.spatial(T.int64(32), ax0_fused + ax0)
                        v1 = T.axis.reduce(num_tokens_including_cache, ax1_fused_0 * T.int64(256) + ax1_fused_1)
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < num_tokens_including_cache)
                        T.reads(lv1645[T.int64(0), v0, T.int64(0), v1])
                        T.writes(T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)])
                        with T.init():
                            T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)], lv1645[T.int64(0), v0, T.int64(0), v1])
        for ax0 in range(T.int64(1)):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax1_fused_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_expsum"):
                        v0 = T.axis.spatial(T.int64(32), ax0_fused + ax0)
                        v1 = T.axis.reduce(num_tokens_including_cache, ax1_fused_0 * T.int64(256) + ax1_fused_1)
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < num_tokens_including_cache)
                        T.reads(lv1645[T.int64(0), v0, T.int64(0), v1], T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)])
                        T.writes(T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)])
                        with T.init():
                            T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)] = T.float32(0)
                        T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)] = T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)] + T.exp(lv1645[T.int64(0), v0, T.int64(0), v1] - T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)])
        for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
            for ax1_0 in T.serial((num_tokens_including_cache + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("compute"):
                    v0 = T.axis.spatial(T.int64(32), ax0_fused)
                    v1 = T.axis.spatial(num_tokens_including_cache, ax1_0 * T.int64(256) + ax1_1)
                    T.where(ax1_0 * T.int64(256) + ax1_1 < num_tokens_including_cache)
                    T.reads(lv1645[T.int64(0), v0, T.int64(0), v1], T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)], T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)])
                    T.writes(compute_intermediate[T.int64(0), v0, T.int64(0), v1])
                    compute_intermediate[T.int64(0), v0, T.int64(0), v1] = T.Cast("float16", T.exp(lv1645[T.int64(0), v0, T.int64(0), v1] - T_softmax_maxelem_shared[T.int64(0), v0, T.int64(0)]) / T_softmax_expsum_shared[T.int64(0), v0, T.int64(0)])
================================================================================
PrimFunc name: fused_fused_decode1_fused_NT_matmul11_cast5
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv1547: T.handle, p_lv1548: T.handle, p_lv1606: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int64()
    lv1547 = T.match_buffer(p_lv1547, (vocab_size, T.int64(1024)), "uint32")
    lv1548 = T.match_buffer(p_lv1548, (vocab_size, T.int64(128)), "float16")
    num_tokens_excluding_cache = T.int64()
    lv1606 = T.match_buffer(p_lv1606, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), num_tokens_excluding_cache, vocab_size))
    # with T.block("root"):
    lv1606_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    decode_intermediate_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (vocab_size + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="shared.dyn")
    lv1606_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.matrix_a")
    decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), (vocab_size + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(4096)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (vocab_size + T.int64(127)) // T.int64(128) * T.int64(128)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (vocab_size + T.int64(127)) // T.int64(128) * T.int64(128)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(1), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding((vocab_size + T.int64(127)) // T.int64(128), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(1), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial((vocab_size + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(64), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv1606_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv1606[v0, v1, v2])
                                            T.writes(lv1606_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv1606_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv1606[v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("decode_intermediate_intermediate_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((vocab_size + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(4096), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv1547[v1, v2 // T.int64(4)], lv1548[v1, v2 // T.int64(32)])
                                            T.writes(decode_intermediate_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            decode_intermediate_intermediate_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < vocab_size, (T.Cast("float16", T.bitwise_and(T.shift_right(lv1547[v1, v2 // T.int64(4)], T.Cast("uint32", v2 % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv1548[v1, v2 // T.int64(32)], T.float16(0))
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv1606_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv1606_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv1606_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv1606_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv1606_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("decode_intermediate_intermediate_reindex_pad_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1_o = T.axis.spatial(T.int64(8) * ((vocab_size + T.int64(127)) // T.int64(128)), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(decode_intermediate_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(decode_intermediate_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(1), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial((vocab_size + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(256), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv1606_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv1606_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv1606_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(decode_intermediate_intermediate_reindex_pad_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(8) * ((vocab_size + T.int64(127)) // T.int64(128)), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial((vocab_size + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
                                    T.writes(compute_intermediate_intermediate[T.int64(0), v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache and v2 < vocab_size:
                                        compute_intermediate_intermediate[T.int64(0), v1, v2] = T.Cast("float32", NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2])
================================================================================
PrimFunc name: rms_norm
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, B: T.Buffer((T.int64(4096),), "float16"), var_rms_norm: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    rms_norm = T.match_buffer(var_rms_norm, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    # with T.block("root"):
    Ared_temp_shared = T.alloc_buffer((T.int64(1), num_tokens_excluding_cache), scope="shared")
    Ared_temp_rf_local = T.alloc_buffer((T.int64(1024), T.int64(1), num_tokens_excluding_cache), scope="local")
    for ax0_fused in T.thread_binding(num_tokens_excluding_cache, thread="blockIdx.x"):
        for ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
            with T.block("Ared_temp_rf_init"):
                vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                T.reads()
                T.writes(Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0])
                Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0] = T.float32(0)
            for ax1_fused_0, u in T.grid(T.int64(4), 1):
                with T.block("Ared_temp_rf_update"):
                    vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                    T.reads(Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0], A[T.int64(0), v0, vax1_fused_0 * T.int64(1024) + vax1_fused_1])
                    T.writes(Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0])
                    Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0] = Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0] + T.Cast("float32", A[T.int64(0), v0, vax1_fused_0 * T.int64(1024) + vax1_fused_1]) * T.Cast("float32", A[T.int64(0), v0, vax1_fused_0 * T.int64(1024) + vax1_fused_1])
        for ax1_fused in range(T.int64(1)):
            for ax0 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("Ared_temp"):
                    vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                    T.reads(Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0])
                    T.writes(Ared_temp_shared[T.int64(0), v0])
                    with T.init():
                        Ared_temp_shared[T.int64(0), v0] = T.float32(0)
                    Ared_temp_shared[T.int64(0), v0] = Ared_temp_shared[T.int64(0), v0] + Ared_temp_rf_local[vax1_fused_1, T.int64(0), v0]
        for ax0_fused_0 in range(T.int64(4)):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("rms_norm"):
                    v0 = T.axis.spatial(num_tokens_excluding_cache, ax0_fused)
                    v1 = T.axis.spatial(T.int64(4096), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(B[v1], A[T.int64(0), v0, v1], Ared_temp_shared[T.int64(0), v0])
                    T.writes(rms_norm[T.int64(0), v0, v1])
                    rms_norm[T.int64(0), v0, v1] = T.Cast("float16", T.Cast("float32", B[v1]) * (T.Cast("float32", A[T.int64(0), v0, v1]) / T.sqrt(Ared_temp_shared[T.int64(0), v0] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05))))
================================================================================
PrimFunc name: fused_fused_decode2_NT_matmul
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(lv3: T.Buffer((T.int64(12288), T.int64(1024)), "uint32"), lv4: T.Buffer((T.int64(12288), T.int64(128)), "float16"), lv1615: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(12288)), "float16")):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    # with T.block("root"):
    NT_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1), T.int64(1), T.int64(12288)), "float16", scope="local")
    NT_matmul_intermediate_rf_local_1 = T.alloc_buffer((T.int64(64), T.int64(1), T.int64(1), T.int64(12288)), "float16", scope="local")
    lv3_local = T.alloc_buffer((T.int64(12288), T.int64(1024)), "uint32", scope="local")
    lv1615_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16", scope="shared")
    for u_fused_ax0_fused_fused_0 in T.thread_binding(T.int64(3072), thread="blockIdx.x"):
        for u_fused_ax0_fused_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
            for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2_0 in T.serial(T.int64(2), annotations={"pragma_unroll_explicit": 256, "pragma_vectorize": 1}):
                        for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                            for ax2_2 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax2_3 in T.vectorized(T.int64(8)):
                                    with T.block("lv1615_shared"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2 = T.axis.spatial(T.int64(4096), ax2_0 * T.int64(2048) + ax2_1 * T.int64(512) + ax2_2 * T.int64(8) + ax2_3)
                                        T.reads(lv1615[v0, v1, v2])
                                        T.writes(lv1615_shared[v0, v1, v2])
                                        lv1615_shared[v0, v1, v2] = lv1615[v0, v1, v2]
                for u_fused_ax0_fused_fused_2_init in range(T.int64(1)):
                    for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init in T.vectorized(T.int64(4)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1_init)
                            v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = T.float16(0)
                for ax1_0_fused_ax1_1_fused_0 in T.serial(T.int64(16), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax0_0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax0_1 in T.vectorized(T.int64(1)):
                            with T.block("lv3_local"):
                                v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + ax0_0 + ax0_1)
                                v1 = T.axis.spatial(T.int64(1024), ax1_0_fused_ax1_1_fused_0 * T.int64(64) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 + ax1)
                                T.reads(lv3[v0, v1])
                                T.writes(lv3_local[v0, v1])
                                lv3_local[v0, v1] = lv3[v0, v1]
                    for u_fused_ax0_fused_fused_2, ax1_0_fused_ax1_1_fused_2 in T.grid(T.int64(1), T.int64(1)):
                        for ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 in T.vectorized(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused = T.axis.spatial(T.int64(256), ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + ax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1)
                                v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + u_fused_ax0_fused_fused_1 + u_fused_ax0_fused_fused_2)
                                vax1_0_fused_ax1_1_fused_2, vax1_0_fused_ax1_1_fused_0 = T.axis.remap("RR", [ax1_0_fused_ax1_1_fused_2, ax1_0_fused_ax1_1_fused_0])
                                T.reads(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0], lv1615_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused], lv3_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], lv4[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
                                T.writes(NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused, T.int64(0), T.int64(0), v0] + lv1615_shared[T.int64(0), T.int64(0), vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused] * ((T.Cast("float16", T.bitwise_and(T.shift_right(lv3_local[v0, vax1_0_fused_ax1_1_fused_0 * T.int64(64) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused // T.int64(4) + vax1_0_fused_ax1_1_fused_2], T.Cast("uint32", (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) % T.int64(4)) * T.uint32(8)), T.uint32(255))) - T.float16(127)) * lv4[v0, (vax1_0_fused_ax1_1_fused_2 * T.int64(4) + vax1_0_fused_ax1_1_fused_0 * T.int64(256) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused) // T.int64(32)])
        for ax2_fused_0 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
            for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_fused_1_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    for ax2_fused_1_1 in T.vectorized(T.int64(1)):
                        with T.block("NT_matmul_rf_init"):
                            vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.spatial(T.int64(64), ax0)
                            v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                            T.reads()
                            T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                            NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = T.float16(0)
                        for ax1 in range(T.int64(4)):
                            with T.block("NT_matmul_rf_update"):
                                vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1 = T.axis.remap("SR", [ax0, ax1])
                                v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + ax2_fused_0 + ax2_fused_1_0 + ax2_fused_1_1)
                                T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0], NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0])
                                T.writes(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                                NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] = NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 * T.int64(4) + vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_1, T.int64(0), T.int64(0), v0]
        for ax1_fused_1 in range(T.int64(1)):
            for ax1_fused_0 in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("NT_matmul"):
                        vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0 = T.axis.reduce(T.int64(64), ax0)
                        v0 = T.axis.spatial(T.int64(12288), u_fused_ax0_fused_fused_0 * T.int64(4) + ax1_fused_0 + ax1_fused_1)
                        T.reads(NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0])
                        T.writes(NT_matmul_intermediate[T.int64(0), T.int64(0), v0])
                        with T.init():
                            NT_matmul_intermediate[T.int64(0), T.int64(0), v0] = T.float16(0)
                        NT_matmul_intermediate[T.int64(0), T.int64(0), v0] = NT_matmul_intermediate[T.int64(0), T.int64(0), v0] + NT_matmul_intermediate_rf_local_1[vax1_0_fused_ax1_1_fused_1_ax1_0_fused_ax1_1_fused_3_fused_0, T.int64(0), T.int64(0), v0]
================================================================================
PrimFunc name: fused_NT_matmul7_divide2_maximum1_minimum1_cast3
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(p_lv28: T.handle, p_lv29: T.handle, p_lv5: T.handle, p_output0: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    lv28 = T.match_buffer(p_lv28, (T.int64(1), T.int64(32), num_tokens_excluding_cache, T.int64(128)), "float16")
    num_tokens_including_cache = T.int64()
    lv29 = T.match_buffer(p_lv29, (T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16")
    lv5 = T.match_buffer(p_lv5, (T.int64(1), T.int64(1), num_tokens_excluding_cache, num_tokens_including_cache), "float16")
    compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), num_tokens_excluding_cache, num_tokens_including_cache))
    # with T.block("root"):
    lv28_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="shared.dyn")
    lv29_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), (num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="shared.dyn")
    lv28_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="wmma.matrix_a")
    lv29_reindex_pad_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(32), (num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128), T.int64(128)), "float16", scope="wmma.matrix_b")
    NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128)), "float16", scope="shared.dyn")
    NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(32), (num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), (num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128)), "float16", scope="wmma.accumulator")
    for ax0 in T.thread_binding(T.int64(32), thread="blockIdx.z"):
        for ax1_0_0_ax2_0_0_fused in T.thread_binding((num_tokens_excluding_cache + T.int64(127)) // T.int64(128), thread="blockIdx.x"):
            for ax1_0_1_ax2_0_1_fused in T.thread_binding((num_tokens_including_cache + T.int64(127)) // T.int64(128), thread="blockIdx.y"):
                for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                    for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_o_init"):
                            v0_o = T.axis.spatial(T.int64(32), ax0)
                            v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3_init)
                            v2_o = T.axis.spatial((num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)
                            T.reads()
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            with T.block("NT_matmul_init_o"):
                                v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))
                                T.reads()
                                T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))
                    for ax3_0_0 in T.serial(T.int64(2), annotations={"software_pipeline_order": [0, 3, 1, 4, 5, 2, 6], "software_pipeline_stage": [0, 0, 0, 0, 0, 1, 1]}):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv28_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(32), ax0)
                                            v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(128), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv28[T.int64(0), v0, v1, v2])
                                            T.writes(lv28_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv28_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_excluding_cache, lv28[T.int64(0), v0, v1, v2], T.float16(0))
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(T.int64(4)):
                                        with T.block("lv29_reindex_pad_shared.dyn"):
                                            v0 = T.axis.spatial(T.int64(32), ax0)
                                            v1 = T.axis.spatial((num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) // T.int64(64))
                                            v2 = T.axis.spatial(T.int64(128), ax3_0_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(128) + ax0_ax1_fused_2 * T.int64(4) + ax0_ax1_fused_3) % T.int64(64))
                                            T.reads(lv29[T.int64(0), v0, v1, v2])
                                            T.writes(lv29_reindex_pad_shared_dyn[v0, v1, v2])
                                            T.block_attr({"buffer_dim_align": [[0, 1, 16, 8]], "double_buffer_scope": 0, "tir.manifest_shared_memory_local_stage": 1})
                                            lv29_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < num_tokens_including_cache, lv29[T.int64(0), v0, v1, v2], T.float16(0))
                        for ax3_0_1 in T.serial(T.int64(4), annotations={"software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 1]}):
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv28_reindex_pad_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(32), ax0)
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(8), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv28_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv28_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv28_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv28_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "row_major")
                            for ax0_0 in T.unroll(T.int64(2)):
                                for ax1_0 in T.unroll(T.int64(1)):
                                    with T.block("lv29_reindex_pad_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(T.int64(32), ax0)
                                        v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_including_cache + T.int64(127)) // T.int64(128)), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)
                                        v2_o = T.axis.spatial(T.int64(8), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)
                                        T.reads(lv29_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        T.writes(lv29_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv29_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                        C = T.match_buffer(lv29_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], "col_major")
                            for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):
                                with T.block("NT_matmul_o_update"):
                                    v0_o = T.axis.spatial(T.int64(32), ax0)
                                    v1_o = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax1_0_3)
                                    v2_o = T.axis.spatial((num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(8), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)
                                    v3_o = T.axis.reduce(T.int64(8), ax3_0_0 * T.int64(4) + ax3_0_1)
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv28_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv29_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                    T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                    with T.block("NT_matmul_o"):
                                        v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))
                                        T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv28_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv29_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                                        A = T.match_buffer(lv28_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                        B = T.match_buffer(lv29_reindex_pad_shared_dyn_wmma_matrix_b[v0_o, v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                        C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                        T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))
                    for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o"):
                            v0_o = T.axis.spatial(T.int64(32), ax0)
                            v1_o = T.axis.spatial(T.int64(8) * ((num_tokens_excluding_cache + T.int64(127)) // T.int64(128)), ax1_0_0_ax2_0_0_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(2) + ax0_0)
                            v2_o = T.axis.spatial(T.int64(8) * ((num_tokens_including_cache + T.int64(127)) // T.int64(128)), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)
                            T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            T.writes(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])
                            A = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                            C = T.match_buffer(NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                            T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation("float16"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], "row_major")
                    for ax0_ax1_fused_0 in range(T.int64(8)):
                        for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                with T.block("NT_matmul_intermediate_reindex_pad_shared.dyn"):
                                    v0 = T.axis.spatial(T.int64(32), ax0)
                                    v1 = T.axis.spatial((num_tokens_excluding_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_0_ax2_0_0_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                    v2 = T.axis.spatial((num_tokens_including_cache + T.int64(127)) // T.int64(128) * T.int64(128), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                    T.reads(NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv5[T.int64(0), T.int64(0), v1, v2])
                                    T.writes(compute_intermediate[T.int64(0), v0, v1, v2])
                                    T.block_attr({"buffer_dim_align": [[0, 1, 16, 4]]})
                                    if v1 < num_tokens_excluding_cache and v2 < num_tokens_including_cache:
                                        compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float32", T.min(T.max(NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] * T.float16(0.088397790055248615), T.float16(-65504)), lv5[T.int64(0), T.int64(0), v1, v2]))
================================================================================
PrimFunc name: transpose5
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_transpose: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_including_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_including_cache, T.int64(32), T.int64(128)), "float16")
    T_transpose = T.match_buffer(var_T_transpose, (T.int64(1), T.int64(32), num_tokens_including_cache, T.int64(128)), "float16")
    # with T.block("root"):
    for ax0_ax1_ax2_fused_0 in T.thread_binding(num_tokens_including_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_transpose"):
                v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // (T.int64(128) * num_tokens_including_cache))
                v1 = T.axis.spatial(num_tokens_including_cache, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(128) * num_tokens_including_cache) // T.int64(128))
                v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(128))
                T.reads(A[T.int64(0), v1, v0, v2])
                T.writes(T_transpose[T.int64(0), v0, v1, v2])
                T_transpose[T.int64(0), v0, v1, v2] = A[T.int64(0), v1, v0, v2]
================================================================================
PrimFunc name: softmax
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_softmax_norm: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    vocab_size = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), T.int64(1), vocab_size))
    T_softmax_norm = T.match_buffer(var_T_softmax_norm, (T.int64(1), T.int64(1), vocab_size))
    # with T.block("root"):
    T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1)), scope="shared")
    T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1)), scope="shared")
    for ax0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
        for ax0 in range(T.int64(1)):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax1_fused_0 in T.serial((vocab_size + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_maxelem"):
                        v0 = T.axis.spatial(T.int64(1), ax0)
                        v1 = T.axis.reduce(vocab_size, ax1_fused_0 * T.int64(256) + ax1_fused_1)
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < vocab_size)
                        T.reads(A[T.int64(0), T.int64(0), v1])
                        T.writes(T_softmax_maxelem_shared[T.int64(0), T.int64(0)])
                        with T.init():
                            T_softmax_maxelem_shared[T.int64(0), T.int64(0)] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[T.int64(0), T.int64(0)] = T.max(T_softmax_maxelem_shared[T.int64(0), T.int64(0)], A[T.int64(0), T.int64(0), v1])
        for ax0 in range(T.int64(1)):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax1_fused_0 in T.serial((vocab_size + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("T_softmax_expsum"):
                        v0 = T.axis.spatial(T.int64(1), ax0)
                        v1 = T.axis.reduce(vocab_size, ax1_fused_0 * T.int64(256) + ax1_fused_1)
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < vocab_size)
                        T.reads(A[T.int64(0), T.int64(0), v1], T_softmax_maxelem_shared[T.int64(0), T.int64(0)])
                        T.writes(T_softmax_expsum_shared[T.int64(0), T.int64(0)])
                        with T.init():
                            T_softmax_expsum_shared[T.int64(0), T.int64(0)] = T.float32(0)
                        T_softmax_expsum_shared[T.int64(0), T.int64(0)] = T_softmax_expsum_shared[T.int64(0), T.int64(0)] + T.exp(A[T.int64(0), T.int64(0), v1] - T_softmax_maxelem_shared[T.int64(0), T.int64(0)])
        for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
            for ax1_0 in T.serial((vocab_size + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("T_softmax_norm"):
                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v1 = T.axis.spatial(vocab_size, ax1_0 * T.int64(256) + ax1_1)
                    T.where(ax1_0 * T.int64(256) + ax1_1 < vocab_size)
                    T.reads(A[T.int64(0), T.int64(0), v1], T_softmax_maxelem_shared[T.int64(0), T.int64(0)], T_softmax_expsum_shared[T.int64(0), T.int64(0)])
                    T.writes(T_softmax_norm[T.int64(0), T.int64(0), v1])
                    T.block_attr({"axis": 2})
                    T_softmax_norm[T.int64(0), T.int64(0), v1] = T.exp(A[T.int64(0), T.int64(0), v1] - T_softmax_maxelem_shared[T.int64(0), T.int64(0)]) / T_softmax_expsum_shared[T.int64(0), T.int64(0)]
================================================================================
PrimFunc name: reshape8
# from tvm.script import tir as T

@T.prim_func(private=True)
def main(var_A: T.handle, var_T_reshape: T.handle):
    T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    num_tokens_excluding_cache = T.int64()
    A = T.match_buffer(var_A, (T.int64(1), num_tokens_excluding_cache, T.int64(32), T.int64(128)), "float16")
    T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), num_tokens_excluding_cache, T.int64(4096)), "float16")
    # with T.block("root"):
    for ax0_ax1_fused_0 in T.thread_binding(num_tokens_excluding_cache * T.int64(4), thread="blockIdx.x"):
        for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
            with T.block("T_reshape"):
                v0 = T.axis.spatial(num_tokens_excluding_cache, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(4096))
                v1 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(4096))
                T.reads(A[T.int64(0), v0, v1 // T.int64(128), v1 % T.int64(128)])
                T.writes(T_reshape[T.int64(0), v0, v1])
                T_reshape[T.int64(0), v0, v1] = A[T.int64(0), v0, v1 // T.int64(128), v1 % T.int64(128)]
================================================================================
